{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.corpus import semcor\n",
    "\n",
    "from data_loader import NO_SENSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package semcor to\n",
      "[nltk_data]     /Users/urisherman/nltk_data...\n",
      "[nltk_data]   Package semcor is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('semcor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(tagged_sentence):\n",
    "    sentence = []\n",
    "    labels = []\n",
    "\n",
    "    def append(chunk, sense):\n",
    "        for s in chunk:\n",
    "            sentence.append(s)\n",
    "            labels.append(sense)\n",
    "\n",
    "    for chunk in tagged_sentence:\n",
    "        if type(chunk) == list:\n",
    "            append(chunk, NO_SENSE)\n",
    "        elif type(chunk) == nltk.tree.Tree:\n",
    "            labeled_words = chunk.leaves()\n",
    "\n",
    "            if len(labeled_words) >= 1:\n",
    "                if type(chunk.label()) == nltk.corpus.reader.wordnet.Lemma:\n",
    "                    # nltk 3.4.5\n",
    "                    str_lbl = chunk.label().synset().name()  # + '_' + chunk.label().name()\n",
    "                elif type(chunk.label()) == str:\n",
    "                    str_lbl = chunk.label()\n",
    "                else:\n",
    "                    raise ValueError(f'Illegal chunk label {chunk.label()}')\n",
    "                append(labeled_words, str_lbl)\n",
    "    return sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37176/37176 [00:50<00:00, 729.38it/s] \n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "senses = []\n",
    "\n",
    "for i, ts in enumerate(tqdm(semcor.tagged_sents(tag='sem'))):\n",
    "    sentence, sense = parse(ts)\n",
    "    sentences.append(sentence)\n",
    "    senses.append(sense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sent_idxs = np.random.permutation(len(sentences))\n",
    "\n",
    "TRAIN_SIZE = 30000\n",
    "\n",
    "with open('./data/sentences.train.jsonl', 'w') as sent_out:\n",
    "    with open('./data/senses.train.jsonl', 'w') as sense_out:\n",
    "        for i in sent_idxs[:TRAIN_SIZE]:\n",
    "            json.dump(sentences[i], sent_out)\n",
    "            sent_out.write('\\n')\n",
    "\n",
    "            json.dump(senses[i], sense_out)\n",
    "            sense_out.write('\\n')\n",
    "    \n",
    "with open('./data/sentences.test.jsonl', 'w') as sent_out:\n",
    "    with open('./data/senses.test.jsonl', 'w') as sense_out:\n",
    "        for i in sent_idxs[TRAIN_SIZE:]:\n",
    "            json.dump(sentences[i], sent_out)\n",
    "            sent_out.write('\\n')\n",
    "\n",
    "            json.dump(senses[i], sense_out)\n",
    "            sense_out.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('sentences.jsonl', 'w') as sent_out:\n",
    "#     with open('senses.jsonl', 'w') as sense_out:\n",
    "#         for i, ts in enumerate(tqdm(semcor.tagged_sents(tag='sem'))):\n",
    "#             sentence, sense = parse(ts)\n",
    "#             json.dump(sentence, sent_out)\n",
    "#             sent_out.write('\\n')\n",
    "            \n",
    "#             json.dump(sense, sense_out)\n",
    "#             sense_out.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no_sense', 'group.n.01', 'group.n.01', 'group.n.01', 'group.n.01', 'state.v.01', 'friday.n.01', 'no_sense', 'probe.n.01', 'no_sense', 'atlanta.n.01', 'no_sense', 'late.s.03', 'primary.n.01', 'primary.n.01', 'produce.v.04', 'no_sense', 'no_sense', 'evidence.n.01', 'no_sense', 'no_sense', 'no_sense', 'abnormality.n.04', 'happen.v.01', 'happen.v.01', 'no_sense']\n",
      "['no_sense', 'jury.n.01', 'far.r.02', 'state.v.01', 'no_sense', 'term.n.02', 'end.n.02', 'presentment.n.01', 'no_sense', 'no_sense', 'group.n.01', 'group.n.01', 'group.n.01', 'no_sense', 'no_sense', 'own.v.01', 'overall.s.02', 'mission.n.03', 'no_sense', 'no_sense', 'election.n.01', 'no_sense', 'no_sense', 'deserve.v.01', 'no_sense', 'praise.n.01', 'no_sense', 'thanks.n.01', 'no_sense', 'no_sense', 'location.n.01', 'location.n.01', 'location.n.01', 'no_sense', 'no_sense', 'no_sense', 'manner.n.01', 'no_sense', 'no_sense', 'no_sense', 'election.n.01', 'no_sense', 'conduct.v.01', 'no_sense']\n",
      "['no_sense', 'september.n.01', 'october.n.01', 'term.n.02', 'jury.n.01', 'no_sense', 'no_sense', 'appoint.v.02', 'no_sense', 'location.n.01', 'person.n.01', 'person.n.01', 'person.n.01', 'person.n.01', 'person.n.01', 'no_sense', 'investigate.v.02', 'report.n.03', 'no_sense', 'potential.a.01', 'no_sense', 'abnormality.n.04', 'no_sense', 'no_sense', 'no_sense', 'hard-fought.s.01', 'primary.n.01', 'no_sense', 'no_sense', 'win.v.01', 'no_sense', 'person.n.01', 'person.n.01', 'person.n.01', 'person.n.01', 'no_sense']\n"
     ]
    }
   ],
   "source": [
    "with open('senses.jsonl', 'r') as data:\n",
    "    head = [next(data) for x in range(3)]\n",
    "    for line in head:\n",
    "        print(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', 'Atlanta', \"'s\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n",
      "['The', 'jury', 'further', 'said', 'in', 'term', 'end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']\n",
      "['The', 'September', 'October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.']\n"
     ]
    }
   ],
   "source": [
    "with open('sentences.jsonl', 'r') as data:\n",
    "    head = [next(data) for x in range(3)]\n",
    "    for line in head:\n",
    "        print(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Pad last 2 dimensions of tensor with (0, 1) -> Adds extra column/row to the right and bottom, whilst copying the values of the current last column/row\n",
    "F.pad(torch.tensor([1,1]), (0,5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tau_nlp",
   "language": "python",
   "name": "tau_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
