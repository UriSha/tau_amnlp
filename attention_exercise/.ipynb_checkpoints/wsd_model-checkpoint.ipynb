{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import data_loader\n",
    "from traineval import train, evaluate\n",
    "import model as model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "The following line of code invokes data_loader and will automatically download and extract the dataset if needed.\n",
    "It instantiates the following variables;\n",
    "* tokens_vocab - the sentence words vocabulary\n",
    "* y_vocab - the labels (senses) vocabulary\n",
    "* datasets - a dictionary with train,dev, and test WSDDataset instances.\n",
    "\n",
    "Use the optional sentence_count kwarg to limit the number of sentences loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets, tokens_vocab, y_vocab = data_loader.load(['train', 'dev'], sentence_count=100)\n",
    "datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['dev']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex2A.1: Implement and train a basic attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.25\n",
    "D = 300\n",
    "\n",
    "m = model.WSDModel(\n",
    "    tokens_vocab.size(), \n",
    "    y_vocab.size(), \n",
    "    D=D, \n",
    "    dropout_prob=dropout\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 8e-5\n",
    "batch_size=100\n",
    "num_epochs=10\n",
    "\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "\n",
    "losses, train_acc, val_acc = train(\n",
    "    m, optimizer, datasets['train'], datasets['dev'], num_epochs=num_epochs, batch_size=batch_size)\n",
    "\n",
    "print(f\"Validation accuracy: {val_acc[-1]:.3f}, Training accuracy:{train_acc[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot loss and train/val accuracy\n",
    "\n",
    "You should be getting ~54% validation accuracy after 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, figsize=(15, 6))\n",
    "\n",
    "axs[0].plot(losses, '-', label='Train Loss');\n",
    "axs[0].legend()\n",
    "axs[1].plot(train_acc, '-o', label='Train Acc');\n",
    "axs[1].plot(val_acc, '-o', label='Val Acc');\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke the attention highlight vizualization to get a feel of what attention is doing.\n",
    "\n",
    "The query token is highlighted green, and the model's attention with pink-blue gradient.\n",
    "In addition, the loss is given a red gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from traineval import higlight_samples\n",
    "\n",
    "higlight_samples(m, datasets['dev'], sample_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the model gives attention to the padded indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex2A.2: Attending Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model.WSDModel(\n",
    "    tokens_vocab.size(), \n",
    "    y_vocab.size(), \n",
    "    D=D, \n",
    "    dropout_prob=dropout\n",
    ").to(device)\n",
    "\n",
    "losses, train_acc, val_acc = train(\n",
    "    m, optimizer, datasets['train'], datasets['dev'], num_epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, figsize=(15, 6))\n",
    "\n",
    "axs[0].plot(losses, '-', label='Train Loss');\n",
    "axs[0].legend()\n",
    "axs[1].plot(train_acc, '-o', label='Train Acc');\n",
    "axs[1].plot(val_acc, '-o', label='Val Acc');\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_vocab.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vocab.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "higlight_samples(m, datasets['dev'], sample_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you like, feel free to inspect more samples, using the api and pandas as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from traineval import evaluate_verbose, highlight\n",
    "\n",
    "pd.set_option('max_columns', 100)\n",
    "\n",
    "eval_df, attention_df = evaluate_verbose(m, datasets['dev'], iter_lim=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show 5 correctly classified samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = np.where(eval_df['y_true'] != eval_df['y_pred'])\n",
    "idxs = list(idxs[0][:5])\n",
    "highlight(eval_df, attention_df, idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show samples of the query word 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = np.where(eval_df['query_token'] == 'left')\n",
    "highlight(eval_df, attention_df, idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex2A.3: Self Attention\n",
    "\n",
    "The method below converts the word level WSDDataset instances to sentence level dataset instances WSDSentencesDataset for self attention mode.\n",
    "\n",
    "Notice how the number of samples now equals number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_datasets = data_loader.WSDSentencesDataset.from_word_datasets(datasets)\n",
    "sa_datasets['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imeplement and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=2e-4\n",
    "dropout = 0.2\n",
    "D=300\n",
    "batch_size=100\n",
    "num_epochs=5\n",
    "\n",
    "m = model.WSDModel(\n",
    "    tokens_vocab.size(), \n",
    "    y_vocab.size(), \n",
    "    D=D, \n",
    "    dropout_prob=dropout\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "\n",
    "losses, train_acc, val_acc = train(\n",
    "    m, optimizer, sa_datasets['train'], sa_datasets['dev'], num_epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, figsize=(15, 6))\n",
    "\n",
    "axs[0].plot(losses, '-', label='Train Loss');\n",
    "axs[0].legend()\n",
    "axs[1].plot(train_acc, '-o', label='Train Acc');\n",
    "axs[1].plot(val_acc, '-o', label='Val Acc');\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 2B: Position-Sensitive Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 2C: Causal Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tau_nlp",
   "language": "python",
   "name": "tau_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
